{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "023bc4bd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-02T15:26:43.736153Z",
     "iopub.status.busy": "2025-12-02T15:26:43.735899Z",
     "iopub.status.idle": "2025-12-02T15:26:45.051862Z",
     "shell.execute_reply": "2025-12-02T15:26:45.051024Z"
    },
    "papermill": {
     "duration": 1.323805,
     "end_time": "2025-12-02T15:26:45.053163",
     "exception": false,
     "start_time": "2025-12-02T15:26:43.729358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/wordtovec-benngoai/model_CUC_20251120175600/out_mal_train_val.pt\n",
      "/kaggle/input/wordtovec-benngoai/model_CUC_20251120175600/path_embedding.pt\n",
      "/kaggle/input/wordtovec-benngoai/model_CUC_20251120175600/out_normal.pt\n",
      "/kaggle/input/wordtovec-benngoai/model_CUC_20251120175600/path_labels.pt\n",
      "/kaggle/input/wordtovec-benngoai/model_CUC_20251120175600/path.pt\n",
      "/kaggle/input/wordtovec-benngoai/model_CUC_20251120175600/out_mal_test.pt\n",
      "/kaggle/input/wordtovec-benngoai/model_CUC_20251120175600/model.pt\n",
      "/kaggle/input/metapath2vec/__results__.html\n",
      "/kaggle/input/metapath2vec/__notebook__.ipynb\n",
      "/kaggle/input/metapath2vec/__output__.json\n",
      "/kaggle/input/metapath2vec/custom.css\n",
      "/kaggle/input/metapath2vec/model_20251120170451/out_mal_train_val.pt\n",
      "/kaggle/input/metapath2vec/model_20251120170451/path_embedding.pt\n",
      "/kaggle/input/metapath2vec/model_20251120170451/out_normal.pt\n",
      "/kaggle/input/metapath2vec/model_20251120170451/path_labels.pt\n",
      "/kaggle/input/metapath2vec/model_20251120170451/path.pt\n",
      "/kaggle/input/metapath2vec/model_20251120170451/path_embedding_2.pt\n",
      "/kaggle/input/metapath2vec/model_20251120170451/out_mal_test.pt\n",
      "/kaggle/input/metapath2vec/model_20251120170451/model.pt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab854499",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T15:26:45.063255Z",
     "iopub.status.busy": "2025-12-02T15:26:45.062931Z",
     "iopub.status.idle": "2025-12-02T15:26:49.776168Z",
     "shell.execute_reply": "2025-12-02T15:26:49.775311Z"
    },
    "papermill": {
     "duration": 4.719744,
     "end_time": "2025-12-02T15:26:49.777619",
     "exception": false,
     "start_time": "2025-12-02T15:26:45.057875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "ray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\r\n",
      "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\r\n",
      "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"protobuf<4.0.0\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f79063a",
   "metadata": {
    "papermill": {
     "duration": 0.004592,
     "end_time": "2025-12-02T15:26:49.812688",
     "exception": false,
     "start_time": "2025-12-02T15:26:49.808096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c4dbdf",
   "metadata": {
    "papermill": {
     "duration": 0.004097,
     "end_time": "2025-12-02T15:26:49.821013",
     "exception": false,
     "start_time": "2025-12-02T15:26:49.816916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355c293",
   "metadata": {
    "papermill": {
     "duration": 0.004152,
     "end_time": "2025-12-02T15:26:49.829326",
     "exception": false,
     "start_time": "2025-12-02T15:26:49.825174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b157575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T15:26:49.987936Z",
     "iopub.status.busy": "2025-12-02T15:26:49.987714Z",
     "iopub.status.idle": "2025-12-02T15:40:46.143139Z",
     "shell.execute_reply": "2025-12-02T15:40:46.142357Z"
    },
    "papermill": {
     "duration": 836.162464,
     "end_time": "2025-12-02T15:40:46.144313",
     "exception": false,
     "start_time": "2025-12-02T15:26:49.981849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 15:26:54.516790: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764689214.711485      20 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764689214.768006      20 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "Loaded path embeddings shape: (31283, 128)\n",
      "Total labels: 31283  | benign: 30850  | malicious: 433\n",
      "\n",
      "====================== TEST 1 - graph_sample_benign_split (ENSEMBLE_MODE=FULL) ======================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764689405.087081      20 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13622 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1764689405.087807      20 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764689410.714333      77 service.cc:148] XLA service 0x7f7e54009770 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1764689410.714755      77 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1764689410.714777      77 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1764689411.199322      77 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1764689414.170086      77 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed = 0 | Method = FULL_ENSEMBLE\n",
      "AUC (score): 0.9611631336405531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       1.00      0.95      0.97      5000\n",
      "   malicious       0.41      0.89      0.57       217\n",
      "\n",
      "    accuracy                           0.94      5217\n",
      "   macro avg       0.70      0.92      0.77      5217\n",
      "weighted avg       0.97      0.94      0.95      5217\n",
      "\n",
      "[[4726  274]\n",
      " [  23  194]]\n",
      "Seed = 1 | Method = FULL_ENSEMBLE\n",
      "AUC (score): 0.9612967741935485\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.99      0.95      0.97      5000\n",
      "   malicious       0.42      0.81      0.56       217\n",
      "\n",
      "    accuracy                           0.95      5217\n",
      "   macro avg       0.71      0.88      0.76      5217\n",
      "weighted avg       0.97      0.95      0.95      5217\n",
      "\n",
      "[[4761  239]\n",
      " [  41  176]]\n",
      "Seed = 2 | Method = FULL_ENSEMBLE\n",
      "AUC (score): 0.9655649769585254\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.99      0.96      0.97      5000\n",
      "   malicious       0.45      0.85      0.59       217\n",
      "\n",
      "    accuracy                           0.95      5217\n",
      "   macro avg       0.72      0.90      0.78      5217\n",
      "weighted avg       0.97      0.95      0.96      5217\n",
      "\n",
      "[[4778  222]\n",
      " [  33  184]]\n",
      "Seed = 3 | Method = FULL_ENSEMBLE\n",
      "AUC (score): 0.9628110599078341\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.99      0.96      0.97      5000\n",
      "   malicious       0.46      0.76      0.57       217\n",
      "\n",
      "    accuracy                           0.95      5217\n",
      "   macro avg       0.72      0.86      0.77      5217\n",
      "weighted avg       0.97      0.95      0.96      5217\n",
      "\n",
      "[[4803  197]\n",
      " [  51  166]]\n",
      "Seed = 4 | Method = FULL_ENSEMBLE\n",
      "AUC (score): 0.9597281105990783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.99      0.95      0.97      5000\n",
      "   malicious       0.42      0.84      0.56       217\n",
      "\n",
      "    accuracy                           0.95      5217\n",
      "   macro avg       0.71      0.89      0.77      5217\n",
      "weighted avg       0.97      0.95      0.95      5217\n",
      "\n",
      "[[4749  251]\n",
      " [  35  182]]\n",
      "Seed = 5 | Method = FULL_ENSEMBLE\n",
      "AUC (score): 0.9609447004608295\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       1.00      0.93      0.96      5000\n",
      "   malicious       0.36      0.94      0.52       217\n",
      "\n",
      "    accuracy                           0.93      5217\n",
      "   macro avg       0.68      0.93      0.74      5217\n",
      "weighted avg       0.97      0.93      0.94      5217\n",
      "\n",
      "[[4632  368]\n",
      " [  14  203]]\n",
      "Seed = 6 | Method = FULL_ENSEMBLE\n",
      "AUC (score): 0.9596820276497696\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.99      0.96      0.97      5000\n",
      "   malicious       0.44      0.80      0.57       217\n",
      "\n",
      "    accuracy                           0.95      5217\n",
      "   macro avg       0.72      0.88      0.77      5217\n",
      "weighted avg       0.97      0.95      0.96      5217\n",
      "\n",
      "[[4784  216]\n",
      " [  44  173]]\n",
      "Seed = 7 | Method = FULL_ENSEMBLE\n",
      "AUC (score): 0.9619235023041475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       1.00      0.94      0.97      5000\n",
      "   malicious       0.40      0.90      0.56       217\n",
      "\n",
      "    accuracy                           0.94      5217\n",
      "   macro avg       0.70      0.92      0.76      5217\n",
      "weighted avg       0.97      0.94      0.95      5217\n",
      "\n",
      "[[4712  288]\n",
      " [  22  195]]\n",
      "Seed = 8 | Method = FULL_ENSEMBLE\n",
      "AUC (score): 0.9636055299539171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.99      0.94      0.97      5000\n",
      "   malicious       0.40      0.87      0.54       217\n",
      "\n",
      "    accuracy                           0.94      5217\n",
      "   macro avg       0.69      0.91      0.76      5217\n",
      "weighted avg       0.97      0.94      0.95      5217\n",
      "\n",
      "[[4711  289]\n",
      " [  28  189]]\n",
      "Seed = 9 | Method = FULL_ENSEMBLE\n",
      "AUC (score): 0.9610903225806452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.99      0.96      0.97      5000\n",
      "   malicious       0.44      0.74      0.55       217\n",
      "\n",
      "    accuracy                           0.95      5217\n",
      "   macro avg       0.71      0.85      0.76      5217\n",
      "weighted avg       0.97      0.95      0.96      5217\n",
      "\n",
      "[[4794  206]\n",
      " [  56  161]]\n",
      "Saved TRAIN samples to train_paths_FULL.csv, shape = (100000, 13)\n",
      "Saved TEST samples to  test_paths_FULL.csv, shape = (52170, 13)\n",
      "Train head:\n",
      "                                split  seed         method  threshold  score  \\\n",
      "0  TEST 1 - graph_sample_benign_split     0  FULL_ENSEMBLE   0.385267    NaN   \n",
      "1  TEST 1 - graph_sample_benign_split     0  FULL_ENSEMBLE   0.385267    NaN   \n",
      "2  TEST 1 - graph_sample_benign_split     0  FULL_ENSEMBLE   0.385267    NaN   \n",
      "3  TEST 1 - graph_sample_benign_split     0  FULL_ENSEMBLE   0.385267    NaN   \n",
      "4  TEST 1 - graph_sample_benign_split     0  FULL_ENSEMBLE   0.385267    NaN   \n",
      "\n",
      "   label_true  label_pred  is_fp  is_fn                   path  node_0  \\\n",
      "0           0         NaN      0      0    [5575, 15480, 6059]    5575   \n",
      "1           0         NaN      0      0       [66, 26102, 202]      66   \n",
      "2           0         NaN      0      0   [11198, 24849, 1872]   11198   \n",
      "3           0         NaN      0      0     [6686, 15480, 833]    6686   \n",
      "4           0         NaN      0      0  [10296, 15480, 10537]   10296   \n",
      "\n",
      "   node_1  node_2  \n",
      "0   15480    6059  \n",
      "1   26102     202  \n",
      "2   24849    1872  \n",
      "3   15480     833  \n",
      "4   15480   10537  \n",
      "Test head:\n",
      "                                split  seed         method  threshold  \\\n",
      "0  TEST 1 - graph_sample_benign_split     0  FULL_ENSEMBLE   0.385267   \n",
      "1  TEST 1 - graph_sample_benign_split     0  FULL_ENSEMBLE   0.385267   \n",
      "2  TEST 1 - graph_sample_benign_split     0  FULL_ENSEMBLE   0.385267   \n",
      "3  TEST 1 - graph_sample_benign_split     0  FULL_ENSEMBLE   0.385267   \n",
      "4  TEST 1 - graph_sample_benign_split     0  FULL_ENSEMBLE   0.385267   \n",
      "\n",
      "      score  label_true  label_pred  is_fp  is_fn                 path  \\\n",
      "0  0.119151           0           0      0      0    [4881, 20778, 66]   \n",
      "1  0.143543           0           0      0      0  [3436, 19701, 3436]   \n",
      "2  0.220768           0           0      0      0    [4358, 15716, 99]   \n",
      "3  0.257728           0           0      0      0    [6128, 15674, 10]   \n",
      "4  0.114550           0           0      0      0    [2359, 18107, 43]   \n",
      "\n",
      "   node_0  node_1  node_2  \n",
      "0    4881   20778      66  \n",
      "1    3436   19701    3436  \n",
      "2    4358   15716      99  \n",
      "3    6128   15674      10  \n",
      "4    2359   18107      43  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# FINAL ENSEMBLE: CLASSICAL (IF/LOF/OCSVM) + DAE/VAE\n",
    "# ====================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, BatchNormalization, Dropout, GaussianNoise, InputLayer\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy.random import permutation\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, roc_auc_score, classification_report,\n",
    "    confusion_matrix, precision_recall_curve\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# LOAD EMBEDDINGS & LABELS\n",
    "embedding_dir = \"/kaggle/input/wordtovec-benngoai/model_CUC_20251120175600\" \n",
    "\n",
    "# Các file sinh ra từ metapath2vec\n",
    "path_list_all        = torch.load(embedding_dir + \"/path.pt\")                # list path (torch.Tensor hoặc tuple)\n",
    "path_embedding_raw   = torch.load(embedding_dir + \"/path_embedding.pt\")      # dict: key=path, value=embedding\n",
    "labels               = torch.load(embedding_dir + \"/path_labels.pt\")\n",
    "out_mal_train_val_dict = torch.load(embedding_dir + \"/out_mal_train_val.pt\")\n",
    "out_mal_test_dict      = torch.load(embedding_dir + \"/out_mal_test.pt\")\n",
    "out_normal_dict        = torch.load(embedding_dir + \"/out_normal.pt\")\n",
    "\n",
    "path_embedding = pd.DataFrame(list(path_embedding_raw.values())).astype(float)\n",
    "out_mal_test_keys = list(out_mal_test_dict.keys())\n",
    "out_mal_val_keys  = list(out_mal_train_val_dict.keys())\n",
    "out_normal_keys   = list(out_normal_dict.keys())\n",
    "\n",
    "print(\"Loaded path embeddings shape:\", path_embedding.shape)\n",
    "print(\"Total labels:\", len(labels),\n",
    "      \" | benign:\", labels.count(0),\n",
    "      \" | malicious:\", labels.count(1))\n",
    "\n",
    "# Mapping index -> path key (benign / malicious)\n",
    "num_benign         = labels.count(0)\n",
    "benign_path_keys   = path_list_all[:num_benign]\n",
    "malicious_path_keys= path_list_all[num_benign:]\n",
    "\n",
    "\n",
    "\n",
    "# SPLIT FUNCTIONS \n",
    "def graph_sample_benign_split(path_embedding, out_mal_test_keys, out_mal_val_keys,\n",
    "                              out_normal_keys, labels, training_sample=10000,\n",
    "                              val_normal_sample=5000, test_normal_sample=5000, seed=10):\n",
    "    \"\"\"\n",
    "    TEST 1 - graph_sample_benign_split\n",
    "    Benign train từ graph; benign val/test từ out_normal_dict; malicious từ graph.\n",
    "    Trả thêm:\n",
    "      - test_paths: list path (benign test + malicious test)\n",
    "      - train_paths: list path cho 10k sample train\n",
    "    \"\"\"\n",
    "    benign_path    = path_embedding.values[:labels.count(0)]\n",
    "    malicious_path = path_embedding.values[labels.count(0):]\n",
    "\n",
    "    validation_ratio = 0.5\n",
    "    test_ratio       = 0.5\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    # benign train\n",
    "    perm = permutation(len(benign_path))\n",
    "    normal_train_data_idx = perm[:training_sample]\n",
    "\n",
    "    # benign val/test out_normal_dict\n",
    "    perm1    = permutation(len(out_normal_keys))\n",
    "    perm_val = perm1[:val_normal_sample]\n",
    "    perm_test= perm1[val_normal_sample:(val_normal_sample + test_normal_sample)]\n",
    "\n",
    "    # malicious val/test malicious_path\n",
    "    perm_mal        = permutation(len(malicious_path))\n",
    "    mal_val_data_idx  = perm_mal[:int(len(perm_mal) * test_ratio)]\n",
    "    mal_test_data_idx = perm_mal[int(len(perm_mal) * test_ratio):]\n",
    "\n",
    "    # ----- BENIGN -----\n",
    "    normal_train_data = np.asarray([benign_path[i] for i in normal_train_data_idx])\n",
    "    # mapping index -> path key\n",
    "    train_paths       = [benign_path_keys[i] for i in normal_train_data_idx]\n",
    "\n",
    "    out_test_keys_sample = [out_normal_keys[i] for i in perm_test]\n",
    "    normal_test_data     = np.asarray([out_normal_dict[i].tolist() for i in out_test_keys_sample])\n",
    "\n",
    "    out_val_keys_sample  = [out_normal_keys[i] for i in perm_val]\n",
    "    normal_val_data      = np.asarray([out_normal_dict[i].tolist() for i in out_val_keys_sample])\n",
    "\n",
    "    # ----- MALICIOUS -----\n",
    "    mal_val_data  = np.asarray([malicious_path[i] for i in mal_val_data_idx])\n",
    "    mal_test_data = np.asarray([malicious_path[i] for i in mal_test_data_idx])\n",
    "\n",
    "    mal_test_paths = [malicious_path_keys[i] for i in mal_test_data_idx]\n",
    "\n",
    "    # ----- SCALE -----\n",
    "    scaler     = MinMaxScaler()\n",
    "    data_scaled= scaler.fit(normal_train_data)\n",
    "\n",
    "    normal_train_data = data_scaled.transform(normal_train_data)\n",
    "    normal_val_data   = data_scaled.transform(normal_val_data)\n",
    "    normal_test_data  = data_scaled.transform(normal_test_data)\n",
    "    mal_val_data      = data_scaled.transform(mal_val_data)\n",
    "    mal_test_data     = data_scaled.transform(mal_test_data)\n",
    "\n",
    "    test_data   = np.concatenate((normal_test_data, mal_test_data), axis=0)\n",
    "    val_data    = np.concatenate((normal_val_data,  mal_val_data), axis=0)\n",
    "    labels_test = [0 for _ in range(len(normal_test_data))] + [1 for _ in range(len(mal_test_data))]\n",
    "    labels_val  = [0 for _ in range(len(normal_val_data))]  + [1 for _ in range(len(mal_val_data))]\n",
    "\n",
    "    normal_test_paths = out_test_keys_sample\n",
    "    test_paths        = normal_test_paths + mal_test_paths\n",
    "\n",
    "    return (normal_train_data, normal_val_data, normal_test_data,\n",
    "            mal_val_data, mal_test_data, test_data, val_data,\n",
    "            labels_test, labels_val, test_paths, train_paths)\n",
    "\n",
    "\n",
    "def log_sample_bengin_split(path_embedding, out_mal_test_keys, out_mal_val_keys,\n",
    "                            out_normal_keys, labels, training_sample=10000,\n",
    "                            val_normal_sample=5000, test_normal_sample=5000, seed=10):\n",
    "    \"\"\"\n",
    "    TEST 2 - log_sample_bengin_split\n",
    "    Benign train/val/test từ out_normal_dict; malicious từ graph.\n",
    "    \"\"\"\n",
    "    benign_path    = path_embedding.values[:labels.count(0)]\n",
    "    malicious_path = path_embedding.values[labels.count(0):]\n",
    "\n",
    "    validation_ratio = 0.5\n",
    "    test_ratio       = 0.5\n",
    "    np.random.seed(seed)\n",
    "    perm = permutation(len(out_normal_keys))\n",
    "\n",
    "    perm_val   = perm[:val_normal_sample]\n",
    "    perm_test  = perm[val_normal_sample:(val_normal_sample + test_normal_sample)]\n",
    "    perm_train = perm[(val_normal_sample + test_normal_sample):\n",
    "                      (val_normal_sample + test_normal_sample + training_sample)]\n",
    "\n",
    "    perm_mal         = permutation(len(malicious_path))\n",
    "    mal_val_data_idx  = perm_mal[:int(len(perm_mal) * test_ratio)]\n",
    "    mal_test_data_idx = perm_mal[int(len(perm_mal) * test_ratio):]\n",
    "\n",
    "    # ----- BENIGN -----\n",
    "    out_train_keys_sample = [out_normal_keys[i] for i in perm_train]\n",
    "    normal_train_data     = np.asarray([out_normal_dict[i].tolist() for i in out_train_keys_sample])\n",
    "    train_paths           = out_train_keys_sample  # path cho train\n",
    "\n",
    "    out_test_keys_sample  = [out_normal_keys[i] for i in perm_test]\n",
    "    normal_test_data      = np.asarray([out_normal_dict[i].tolist() for i in out_test_keys_sample])\n",
    "\n",
    "    out_val_keys_sample   = [out_normal_keys[i] for i in perm_val]\n",
    "    normal_val_data       = np.asarray([out_normal_dict[i].tolist() for i in out_val_keys_sample])\n",
    "\n",
    "    # ----- MALICIOUS -----\n",
    "    mal_val_data  = np.asarray([malicious_path[i] for i in mal_val_data_idx])\n",
    "    mal_test_data = np.asarray([malicious_path[i] for i in mal_test_data_idx])\n",
    "\n",
    "    mal_test_paths = [malicious_path_keys[i] for i in mal_test_data_idx]\n",
    "\n",
    "    # ----- SCALE -----\n",
    "    scaler      = MinMaxScaler()\n",
    "    data_scaled = scaler.fit(normal_train_data)\n",
    "\n",
    "    normal_train_data = data_scaled.transform(normal_train_data)\n",
    "    normal_val_data   = data_scaled.transform(normal_val_data)\n",
    "    normal_test_data  = data_scaled.transform(normal_test_data)\n",
    "    mal_val_data      = data_scaled.transform(mal_val_data)\n",
    "    mal_test_data     = data_scaled.transform(mal_test_data)\n",
    "\n",
    "    test_data   = np.concatenate((normal_test_data, mal_test_data), axis=0)\n",
    "    val_data    = np.concatenate((normal_val_data,  mal_val_data), axis=0)\n",
    "    labels_test = [0 for _ in range(len(normal_test_data))] + [1 for _ in range(len(mal_test_data))]\n",
    "    labels_val  = [0 for _ in range(len(normal_val_data))]  + [1 for _ in range(len(mal_val_data))]\n",
    "\n",
    "    normal_test_paths = out_test_keys_sample\n",
    "    test_paths        = normal_test_paths + mal_test_paths\n",
    "\n",
    "    return (normal_train_data, normal_val_data, normal_test_data,\n",
    "            mal_val_data, mal_test_data, test_data, val_data,\n",
    "            labels_test, labels_val, test_paths, train_paths)\n",
    "\n",
    "\n",
    "def graph_sample_bengin_day_split(path_embedding, out_mal_test_keys, out_mal_val_keys,\n",
    "                                  out_normal_keys, labels, training_sample=10000,\n",
    "                                  val_normal_sample=5000, test_normal_sample=5000, seed=10):\n",
    "    \"\"\"\n",
    "    TEST 3 - graph_sample_bengin_day_split\n",
    "    Benign train từ graph; benign val/test từ out_normal_dict;\n",
    "    malicious từ out_mal_train_val_dict & out_mal_test_dict.\n",
    "    \"\"\"\n",
    "    benign_path = path_embedding.values[:labels.count(0)]\n",
    "    np.random.seed(seed)\n",
    "    perm = permutation(len(out_normal_keys))\n",
    "\n",
    "    perm_val   = perm[:val_normal_sample]\n",
    "    perm_test  = perm[(val_normal_sample):(val_normal_sample + test_normal_sample)]\n",
    "    perm_train = perm[(val_normal_sample + test_normal_sample):\n",
    "                      (training_sample + val_normal_sample + test_normal_sample)]\n",
    "\n",
    "    perm_benign          = permutation(len(benign_path))\n",
    "    normal_train_data_idx= perm_benign[:training_sample]\n",
    "\n",
    "    normal_train_data = np.asarray([benign_path[i] for i in normal_train_data_idx])\n",
    "    train_paths       = [benign_path_keys[i] for i in normal_train_data_idx]\n",
    "\n",
    "    out_test_keys_sample = [out_normal_keys[i] for i in perm_test]\n",
    "    normal_test_data     = np.asarray([out_normal_dict[i].tolist() for i in out_test_keys_sample])\n",
    "\n",
    "    out_val_keys_sample  = [out_normal_keys[i] for i in perm_val]\n",
    "    normal_val_data      = np.asarray([out_normal_dict[i].tolist() for i in out_val_keys_sample])\n",
    "\n",
    "    mal_val_data  = np.asarray([out_mal_train_val_dict[i].tolist() for i in out_mal_train_val_dict])\n",
    "    mal_val_paths = list(out_mal_train_val_dict.keys())\n",
    "\n",
    "    mal_test_keys = list(out_mal_test_dict.keys())\n",
    "    mal_test_data = np.asarray([out_mal_test_dict[i].tolist() for i in mal_test_keys])\n",
    "\n",
    "    scaler      = MinMaxScaler()\n",
    "    data_scaled = scaler.fit(normal_train_data)\n",
    "\n",
    "    normal_train_data = data_scaled.transform(normal_train_data)\n",
    "    normal_val_data   = data_scaled.transform(normal_val_data)\n",
    "    normal_test_data  = data_scaled.transform(normal_test_data)\n",
    "    mal_val_data      = data_scaled.transform(mal_val_data)\n",
    "    mal_test_data     = data_scaled.transform(mal_test_data)\n",
    "\n",
    "    test_data   = np.concatenate((normal_test_data, mal_test_data), axis=0)\n",
    "    val_data    = np.concatenate((normal_val_data,  mal_val_data), axis=0)\n",
    "    labels_test = [0 for _ in range(len(normal_test_data))] + [1 for _ in range(len(mal_test_data))]\n",
    "    labels_val  = [0 for _ in range(len(normal_val_data))]  + [1 for _ in range(len(mal_val_data))]\n",
    "\n",
    "    normal_test_paths = out_test_keys_sample\n",
    "    mal_test_paths    = mal_test_keys\n",
    "    test_paths        = normal_test_paths + mal_test_paths\n",
    "\n",
    "    return (normal_train_data, normal_val_data, normal_test_data,\n",
    "            mal_val_data, mal_test_data, test_data, val_data,\n",
    "            labels_test, labels_val, test_paths, train_paths)\n",
    "\n",
    "\n",
    "def log_sample_bengin_day_split(path_embedding, out_mal_test_keys, out_mal_val_keys,\n",
    "                                out_normal_keys, labels, training_sample=10000,\n",
    "                                val_normal_sample=5000, test_normal_sample=5000, seed=10):\n",
    "    \"\"\"\n",
    "    TEST 4 - log_sample_bengin_day_split\n",
    "    Benign train/val/test từ out_normal_dict;\n",
    "    malicious từ out_mal_train_val_dict & out_mal_test_dict.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    perm = permutation(len(out_normal_keys))\n",
    "\n",
    "    perm_val   = perm[:val_normal_sample]\n",
    "    perm_test  = perm[(val_normal_sample):(val_normal_sample + test_normal_sample)]\n",
    "    perm_train = perm[(val_normal_sample + test_normal_sample):\n",
    "                      (training_sample + val_normal_sample + test_normal_sample)]\n",
    "\n",
    "    out_train_keys_sample = [out_normal_keys[i] for i in perm_train]\n",
    "    normal_train_data     = np.asarray([out_normal_dict[i].tolist() for i in out_train_keys_sample])\n",
    "    train_paths           = out_train_keys_sample\n",
    "\n",
    "    out_test_keys_sample  = [out_normal_keys[i] for i in perm_test]\n",
    "    normal_test_data      = np.asarray([out_normal_dict[i].tolist() for i in out_test_keys_sample])\n",
    "\n",
    "    out_val_keys_sample   = [out_normal_keys[i] for i in perm_val]\n",
    "    normal_val_data       = np.asarray([out_normal_dict[i].tolist() for i in out_val_keys_sample])\n",
    "\n",
    "    mal_val_data  = np.asarray([out_mal_train_val_dict[i].tolist() for i in out_mal_train_val_dict])\n",
    "    mal_val_paths = list(out_mal_train_val_dict.keys())\n",
    "\n",
    "    mal_test_keys = list(out_mal_test_dict.keys())\n",
    "    mal_test_data = np.asarray([out_mal_test_dict[i].tolist() for i in mal_test_keys])\n",
    "\n",
    "    scaler      = MinMaxScaler()\n",
    "    data_scaled = scaler.fit(normal_train_data)\n",
    "\n",
    "    normal_train_data = data_scaled.transform(normal_train_data)\n",
    "    normal_val_data   = data_scaled.transform(normal_val_data)\n",
    "    normal_test_data  = data_scaled.transform(normal_test_data)\n",
    "    mal_val_data      = data_scaled.transform(mal_val_data)\n",
    "    mal_test_data     = data_scaled.transform(mal_test_data)\n",
    "\n",
    "    test_data   = np.concatenate((normal_test_data, mal_test_data), axis=0)\n",
    "    val_data    = np.concatenate((normal_val_data,  mal_val_data), axis=0)\n",
    "    labels_test = [0 for _ in range(len(normal_test_data))] + [1 for _ in range(len(mal_test_data))]\n",
    "    labels_val  = [0 for _ in range(len(normal_val_data))]  + [1 for _ in range(len(mal_val_data))]\n",
    "\n",
    "    normal_test_paths = out_test_keys_sample\n",
    "    mal_test_paths    = mal_test_keys\n",
    "    test_paths        = normal_test_paths + mal_test_paths\n",
    "\n",
    "    return (normal_train_data, normal_val_data, normal_test_data,\n",
    "            mal_val_data, mal_test_data, test_data, val_data,\n",
    "            labels_test, labels_val, test_paths, train_paths)\n",
    "\n",
    "\n",
    "# CLASSICAL DETECTORS\n",
    "\n",
    "def get_scores_iforest(train_normal, val_data, test_data, seed):\n",
    "    clf = IsolationForest(\n",
    "        n_estimators=200,\n",
    "        contamination=\"auto\",\n",
    "        random_state=seed,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf.fit(train_normal)\n",
    "\n",
    "    val_raw  = -clf.score_samples(val_data)   \n",
    "    test_raw = -clf.score_samples(test_data)\n",
    "    return val_raw, test_raw\n",
    "\n",
    "\n",
    "def get_scores_lof(train_normal, val_data, test_data, seed):\n",
    "    clf = LocalOutlierFactor(\n",
    "        n_neighbors=20,\n",
    "        novelty=True,\n",
    "        contamination=\"auto\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf.fit(train_normal)\n",
    "\n",
    "    val_raw  = -clf.score_samples(val_data)\n",
    "    test_raw = -clf.score_samples(test_data)\n",
    "    return val_raw, test_raw\n",
    "\n",
    "\n",
    "def get_scores_ocsvm(train_normal, val_data, test_data, seed):\n",
    "    clf = OneClassSVM(\n",
    "        kernel=\"rbf\",\n",
    "        gamma=\"scale\",\n",
    "        nu=0.05,\n",
    "    )\n",
    "    clf.fit(train_normal)\n",
    "\n",
    "    val_raw  = -clf.decision_function(val_data)\n",
    "    test_raw = -clf.decision_function(test_data)\n",
    "    return val_raw, test_raw\n",
    "\n",
    "\n",
    "# DENOISING AE & VAE\n",
    "class DenoisingAutoencoder(Model):\n",
    "    def __init__(self, input_dim, latent_dim=16, noise_std=0.05, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Sequential([\n",
    "            InputLayer(shape=(input_dim,)),\n",
    "            GaussianNoise(noise_std),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(64, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(latent_dim, activation='relu'),\n",
    "        ])\n",
    "\n",
    "        self.decoder = Sequential([\n",
    "            InputLayer(shape=(latent_dim,)),\n",
    "            Dense(64, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(input_dim, activation='linear'),\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        z = self.encoder(x, training=training)\n",
    "        x_hat = self.decoder(z, training=training)\n",
    "        return x_hat\n",
    "\n",
    "\n",
    "class VAE(Model):\n",
    "    def __init__(self, input_dim, latent_dim=16, beta=1.0):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.beta       = beta\n",
    "\n",
    "        # Encoder network\n",
    "        self.encoder_net = Sequential([\n",
    "            InputLayer(shape=(input_dim,)),\n",
    "            Dense(128, activation=\"relu\"),\n",
    "            Dense(64, activation=\"relu\"),\n",
    "        ])\n",
    "        self.z_mean_layer    = Dense(latent_dim)\n",
    "        self.z_log_var_layer = Dense(latent_dim)\n",
    "\n",
    "        # Decoder network\n",
    "        self.decoder_net = Sequential([\n",
    "            InputLayer(shape=(latent_dim,)),\n",
    "            Dense(64, activation=\"relu\"),\n",
    "            Dense(128, activation=\"relu\"),\n",
    "            Dense(input_dim, activation=\"linear\"),\n",
    "        ])\n",
    "\n",
    "    def encode(self, x):\n",
    "        h        = self.encoder_net(x)\n",
    "        z_mean   = self.z_mean_layer(h)\n",
    "        z_log_var= self.z_log_var_layer(h)\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "    def reparameterize(self, z_mean, z_log_var):\n",
    "        eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder_net(z)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encode(x)\n",
    "            z = self.reparameterize(z_mean, z_log_var)\n",
    "            reconstruction = self.decode(z)\n",
    "\n",
    "            # Reconstruction loss (MAE)\n",
    "            recon_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(tf.abs(x - reconstruction), axis=1)\n",
    "            )\n",
    "            # KL divergence\n",
    "            kl_loss = -0.5 * tf.reduce_mean(\n",
    "                tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "            )\n",
    "            total_loss = recon_loss + self.beta * kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"recon_loss\": recon_loss,\n",
    "            \"kl_loss\": kl_loss\n",
    "        }\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        z_mean, z_log_var = self.encode(x)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        return self.decode(z)\n",
    "\n",
    "\n",
    "# HELPER: ERROR, SCORE, THRESHOLD, PATH FORMAT\n",
    "def compute_reconstruction_error(model, x):\n",
    "    recon  = model.predict(x, verbose=0)\n",
    "    errors = np.mean(np.abs(x - recon), axis=1)\n",
    "    return errors\n",
    "\n",
    "\n",
    "def normalize_scores(errors):\n",
    "    e_min = errors.min()\n",
    "    e_max = errors.max()\n",
    "    if e_max == e_min:\n",
    "        return np.zeros_like(errors)\n",
    "    return (errors - e_min) / (e_max - e_min)\n",
    "\n",
    "\n",
    "def select_threshold(y_val, scores_val, strategy=\"AUC\"):\n",
    "    y_val = np.array(y_val)\n",
    "\n",
    "    if strategy.upper() == \"AUC\":\n",
    "        fpr, tpr, thresholds = roc_curve(y_val, scores_val)\n",
    "        gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "        ix = np.argmax(gmeans)\n",
    "        return thresholds[ix]\n",
    "\n",
    "    elif strategy.upper() == \"F1\":\n",
    "        precision, recall, thresholds = precision_recall_curve(y_val, scores_val)\n",
    "        f1_scores = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1] + 1e-8)\n",
    "        f1_scores = np.nan_to_num(f1_scores)\n",
    "        ix = np.argmax(f1_scores)\n",
    "        return thresholds[ix]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"strategy must be 'AUC' or 'F1'\")\n",
    "\n",
    "\n",
    "def path_to_str(path_key):\n",
    "    \"\"\"\n",
    "    Chuyển path (tensor/tuple/list) -> chuỗi dạng vector\n",
    "    \"\"\"\n",
    "    if path_key is None:\n",
    "        return None\n",
    "    if isinstance(path_key, torch.Tensor):\n",
    "        ids = path_key.tolist()\n",
    "    else:\n",
    "        ids = list(path_key)\n",
    "    return \"[\" + \", \".join(str(int(x)) for x in ids) + \"]\"\n",
    "\n",
    "\n",
    "# EXPERIMENT RUNNER + FULL ENSEMBLE\n",
    "training_sample    = 10000\n",
    "val_normal_sample  = 5000\n",
    "test_normal_sample = 5000\n",
    "\n",
    "best_strat = \"F1\"          \n",
    "\n",
    "# ENSEMBLE_MODE:\n",
    "#  - \"CLS_ONLY\"    : Only IF+LOF+OCSVM\n",
    "#  - \"DEEP_ONLY\"   : Only DAE+VAE\n",
    "#  - \"FULL\"        : All (IF, LOF, OCSVM, DAE, VAE)\n",
    "ENSEMBLE_MODE = \"FULL\"\n",
    "\n",
    "NUM_SEEDS = 10   \n",
    "\n",
    "train_rows = []   \n",
    "test_rows  = []   \n",
    "\n",
    "\n",
    "def run_experiment(split_func, split_name):\n",
    "    global train_rows, test_rows\n",
    "\n",
    "    print(f\"\\n====================== {split_name} (ENSEMBLE_MODE={ENSEMBLE_MODE}) ======================\\n\")\n",
    "    for seed in range(NUM_SEEDS):\n",
    "        (normal_train_data, normal_val_data, normal_test_data,\n",
    "         mal_val_data, mal_test_data, test_data, val_data,\n",
    "         labels_test, labels_val, test_paths, train_paths) = split_func(\n",
    "            path_embedding, out_mal_test_keys, out_mal_val_keys,\n",
    "            out_normal_keys, labels,\n",
    "            training_sample=training_sample,\n",
    "            val_normal_sample=val_normal_sample,\n",
    "            test_normal_sample=test_normal_sample,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        input_dim = normal_train_data.shape[1]\n",
    "\n",
    "        use_classical = ENSEMBLE_MODE in [\"CLS_ONLY\", \"FULL\"]\n",
    "        use_deep      = ENSEMBLE_MODE in [\"DEEP_ONLY\", \"FULL\"]\n",
    "\n",
    "        # ----- CLASSICAL SCORES (IF/LOF/OCSVM) -----\n",
    "        if use_classical:\n",
    "            val_if,  test_if  = get_scores_iforest(normal_train_data, val_data,  test_data, seed)\n",
    "            val_lof, test_lof = get_scores_lof(    normal_train_data, val_data,  test_data, seed)\n",
    "            val_oc,  test_oc  = get_scores_ocsvm(  normal_train_data, val_data,  test_data, seed)\n",
    "\n",
    "            val_if   = normalize_scores(val_if)\n",
    "            test_if  = normalize_scores(test_if)\n",
    "            val_lof  = normalize_scores(val_lof)\n",
    "            test_lof = normalize_scores(test_lof)\n",
    "            val_oc   = normalize_scores(val_oc)\n",
    "            test_oc  = normalize_scores(test_oc)\n",
    "\n",
    "        # ----- DAE + VAE -----\n",
    "        if use_deep:\n",
    "            # DAE\n",
    "            dae = DenoisingAutoencoder(input_dim=input_dim)\n",
    "            dae.compile(optimizer='adam', loss='mae')\n",
    "            es_dae = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=20,\n",
    "                restore_best_weights=True,\n",
    "                verbose=0,\n",
    "                mode='min'\n",
    "            )\n",
    "            dae.fit(\n",
    "                normal_train_data, normal_train_data,\n",
    "                epochs=500,\n",
    "                batch_size=128,\n",
    "                validation_data=(normal_val_data, normal_val_data),\n",
    "                shuffle=True,\n",
    "                callbacks=[es_dae],\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            val_err_dae  = compute_reconstruction_error(dae,  val_data)\n",
    "            test_err_dae = compute_reconstruction_error(dae,  test_data)\n",
    "            val_dae  = normalize_scores(val_err_dae)\n",
    "            test_dae = normalize_scores(test_err_dae)\n",
    "\n",
    "            # VAE\n",
    "            vae = VAE(input_dim=input_dim, latent_dim=16, beta=1.0)\n",
    "            vae.compile(optimizer=tf.keras.optimizers.Adam(1e-3))\n",
    "            vae.fit(\n",
    "                normal_train_data,\n",
    "                epochs=80,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            val_err_vae  = compute_reconstruction_error(vae,  val_data)\n",
    "            test_err_vae = compute_reconstruction_error(vae,  test_data)\n",
    "            val_vae  = normalize_scores(val_err_vae)\n",
    "            test_vae = normalize_scores(test_err_vae)\n",
    "\n",
    "        # ======= MERGE SCORE BY ENSEMBLE_MODE =======\n",
    "        if ENSEMBLE_MODE == \"CLS_ONLY\":\n",
    "            val_scores  = (val_if + val_lof + val_oc) / 3.0\n",
    "            test_scores = (test_if + test_lof + test_oc) / 3.0\n",
    "            method_name = \"CLS_ENSEMBLE\"\n",
    "\n",
    "        elif ENSEMBLE_MODE == \"DEEP_ONLY\":\n",
    "            val_scores  = 0.5 * (val_dae + val_vae)\n",
    "            test_scores = 0.5 * (test_dae + test_vae)\n",
    "            method_name = \"DEEP_ENSEMBLE\"\n",
    "\n",
    "        elif ENSEMBLE_MODE == \"FULL\":\n",
    "            comp_list_val  = []\n",
    "            comp_list_test = []\n",
    "            if use_classical:\n",
    "                comp_list_val.extend([val_if, val_lof, val_oc])\n",
    "                comp_list_test.extend([test_if, test_lof, test_oc])\n",
    "            if use_deep:\n",
    "                comp_list_val.extend([val_dae, val_vae])\n",
    "                comp_list_test.extend([test_dae, test_vae])\n",
    "\n",
    "            val_scores  = np.mean(np.vstack(comp_list_val),  axis=0)\n",
    "            test_scores = np.mean(np.vstack(comp_list_test), axis=0)\n",
    "            method_name = \"FULL_ENSEMBLE\"\n",
    "        else:\n",
    "            raise ValueError(\"ENSEMBLE_MODE must be 'CLS_ONLY', 'DEEP_ONLY', or 'FULL'\")\n",
    "\n",
    "        # ----- Threshold on validation -----\n",
    "        threshold      = select_threshold(labels_val, val_scores, strategy=best_strat)\n",
    "        test_scores_np = np.array(test_scores)\n",
    "        labels_test_np = np.array(labels_test)\n",
    "        y_pred         = (test_scores_np > threshold).astype(int)\n",
    "\n",
    "        print(f\"Seed = {seed} | Method = {method_name}\")\n",
    "        print(\"AUC (score):\", roc_auc_score(labels_test_np, test_scores_np))\n",
    "        print(classification_report(labels_test_np, y_pred,\n",
    "                                    labels=[0, 1],\n",
    "                                    target_names=['benign', 'malicious']))\n",
    "        print(confusion_matrix(labels_test_np, y_pred))\n",
    "\n",
    "        for idx in range(len(test_scores_np)):\n",
    "            path_key = test_paths[idx]\n",
    "            path_str = path_to_str(path_key)\n",
    "\n",
    "            true_label = int(labels_test_np[idx])\n",
    "            pred_label = int(y_pred[idx])\n",
    "\n",
    "            row = {\n",
    "                \"split\": split_name,\n",
    "                \"seed\": seed,\n",
    "                \"method\": method_name,\n",
    "                \"threshold\": float(threshold),\n",
    "                \"score\": float(test_scores_np[idx]),\n",
    "                \"label_true\": true_label,\n",
    "                \"label_pred\": pred_label,\n",
    "                \"is_fp\": int(true_label == 0 and pred_label == 1),\n",
    "                \"is_fn\": int(true_label == 1 and pred_label == 0),\n",
    "                \"path\": path_str,\n",
    "            }\n",
    "\n",
    "            if path_key is not None:\n",
    "                if isinstance(path_key, torch.Tensor):\n",
    "                    ids = path_key.tolist()\n",
    "                else:\n",
    "                    ids = list(path_key)\n",
    "                for j, node_id in enumerate(ids):\n",
    "                    row[f\"node_{j}\"] = int(node_id)\n",
    "\n",
    "            test_rows.append(row)\n",
    "\n",
    "        \n",
    "        for path_key in train_paths:\n",
    "            path_str = path_to_str(path_key)\n",
    "            row = {\n",
    "                \"split\": split_name,\n",
    "                \"seed\": seed,\n",
    "                \"method\": method_name,\n",
    "                \"threshold\": float(threshold), \n",
    "                \"score\": np.nan,\n",
    "                \"label_true\": 0,\n",
    "                \"label_pred\": np.nan,\n",
    "                \"is_fp\": 0,\n",
    "                \"is_fn\": 0,\n",
    "                \"path\": path_str,\n",
    "            }\n",
    "            if path_key is not None:\n",
    "                if isinstance(path_key, torch.Tensor):\n",
    "                    ids = path_key.tolist()\n",
    "                else:\n",
    "                    ids = list(path_key)\n",
    "                for j, node_id in enumerate(ids):\n",
    "                    row[f\"node_{j}\"] = int(node_id)\n",
    "\n",
    "            train_rows.append(row)\n",
    "\n",
    "\n",
    "# RUN TESTS\n",
    "run_experiment(graph_sample_benign_split,     \"TEST 1 - graph_sample_benign_split\")\n",
    "# run_experiment(log_sample_bengin_split,       \"TEST 2 - log_sample_bengin_split\")\n",
    "# run_experiment(graph_sample_bengin_day_split, \"TEST 3 - graph_sample_bengin_day_split\")\n",
    "# run_experiment(log_sample_bengin_day_split,   \"TEST 4 - log_sample_bengin_day_split\")\n",
    "\n",
    "train_df = pd.DataFrame(train_rows)\n",
    "test_df  = pd.DataFrame(test_rows)\n",
    "\n",
    "train_csv = f\"train_paths_{ENSEMBLE_MODE}.csv\"\n",
    "test_csv  = f\"test_paths_{ENSEMBLE_MODE}.csv\"\n",
    "\n",
    "train_df.to_csv(train_csv, index=False)\n",
    "test_df.to_csv(test_csv, index=False)\n",
    "\n",
    "print(f\"Saved TRAIN samples to {train_csv}, shape = {train_df.shape}\")\n",
    "print(f\"Saved TEST samples to  {test_csv}, shape = {test_df.shape}\")\n",
    "print(\"Train head:\")\n",
    "print(train_df.head())\n",
    "print(\"Test head:\")\n",
    "print(test_df.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8792276,
     "sourceId": 13807944,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 280363206,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 848.900529,
   "end_time": "2025-12-02T15:40:49.168048",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-02T15:26:40.267519",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
